\documentclass[a4]{article}

\usepackage{amsmath}
\usepackage{calc}
\usepackage{cite}
\usepackage{tikz}
\usetikzlibrary{calc, shapes, arrows, chains}

\def\edcomm#1#2{%
  \framebox{\textbf{#1:} #2}
}


\usepackage{semantic}

\begin{document}

%{{{ Introduction
\section{Introduction}
%}}}

%{{{ Model

\section{Model}

\subsection{Existing SCOOP models}
The existing SCOOP model is defined operationally,
either informally as a prose description \cite{nienaltowski:2007:SCOOP},
or more formally as a 
structural operational semantics \cite{morandi-et-al:2013:prototyping}.
These descriptions are highly detailed and allow a complete
picture about what SCOOP is doing under the covers.

\subsection{One more model}

Our model for SCOOP processors consists of both a past and a future:
completed actions are remembered,
and yet-to-be-executed actions stored.
Each is represented by a list of actions:

\def\proc{\mathit{Processor}}
\def\act{\mathit{Action}}

\[
\proc = \act^{*} \times \act^{*}
\]

For example, a processor which has been requested to calculate
two pieces of data may be at some intermediate state where
only the first calculation has completed:

\[
\left(\left[\mathit{calc_1}\right],\left[\mathit{calc_2}\right] \right)
\]

For example, the following small program that first reserves access to
\texttt{x} and \texttt{y} then makes several calls on them modifies
the queues in the following way (with post state indicated with $\mathit{'}$):

\def\dplus{+ \hskip -0.75em +}
\begin{center}
\begin{minipage}{0.7\linewidth}
\texttt{separate x, y}

\hfill $\mathit{access} = \{x,y\}$

\hskip 2em \texttt{x.foo}

\hfill $\mathit{x.future'} = \mathit{x.future} \dplus  [\mathit{foo}]$

\hskip 2em \texttt{y.bar}

\hfill $\mathit{y.future'} = \mathit{y.future} \dplus  [\mathit{bar}]$

\hskip 2em \texttt{n := x.baz}
\setlength{\abovedisplayskip}{0pt}
\setlength{\belowdisplayskip}{0pt}
\setlength\arraycolsep{1pt}
\begin{flalign*}
&&\mathit{x.past'} &= \mathit{x.past}  \dplus \mathit{x.future} \dplus [\mathit{baz}]\\
&&\mathit{x.future'} &= []
\end{flalign*}
\texttt{end}

\hfill $\mathit{access} = \{\}$

\end{minipage}
\end{center}

In the first step, the separate processors that \texttt{x} and \texttt{y}
live on are reserved.
Then, the call \texttt{foo} is appended to the list of actions
to be completed by the processor of \texttt{x};
likewise for the call \texttt{bar} on \texttt{y}.


If we take the above the above program, $\mathit{P}$,
we can produce a trace of dispatches:

\[
\mathit{dispatches}\left(\mathit{P}\right) =
  \left[\mathtt{x.foo}, \mathtt{y.bar}, \mathtt{x.baz}\right]
\]

Additionally, we can also project the trace on a particular processor.
Here,

\begin{flalign*}
  \mathit{dispatches}_x\left(\mathit{P}\right) &=
    \left[\mathtt{x.foo}, \mathtt{x.baz}\right] \\
  \mathit{dispatches}_y\left(\mathit{P}\right) &=
    \left[\mathtt{y.bar}\right] \\
\end{flalign*}

Given these projections,
the desired property of the execution is that the calls are
applied in the order they are logged.
\edcomm{SW}{Define apply and logged earlier.}
If we have a global ordering on events $<$
and ordering of events the heap of processor \texttt{x},
then we require the following holds

\[
\mathtt{foo} < \mathtt{baz} \wedge
\neg \exists a . \mathtt{foo} <_x a <_x \mathtt{baz}
\]

Basically, \texttt{foo} must occur before \texttt{baz} and
there must be no operations on the heap of processor \texttt{x}
that occur between them.
Therefore every sequence of dispatches to a particular processor
places a restriction on the global ordering of their application,
as well as a local restricted ordering on the local heap of that processor.
The local ordering is important to reason that all calls logged
to a separate processor will happen without any
intervening modifications to the state on that processor.

\subsection{Guarantees}
While the operational model is useful to understand what may be happening
in the runtime of SCOOP,
it is not as suitable to understanding what the user's program
is doing.
For this, we would like to have an axiomatic semantics that describes
what one can rely on in terms of the program state and separate processors.
We define our axiomatic semantics on a simplified syntax.

% \[
% \inference[base]{}{\{P\}a\{Q\}}
% \inference[sep]{}
%\]

The primary guarantee that the SCOOP model provides is that calls to a
processor $p$ that occur while a processor $q$ has the lock
will be executed in order.
This enables processor $q$ to have
a stream of pre-post condition reasoning that
it can influence from the ``outside'' by logging new calls on $p$.
Likewise $q$ can also manipulate a stream of operations on other
processors that it has locked in parallel with $p$.
Essentially $q$ functions as a coordinator between itself
and the processors it has locked,
dispatching calls as it desires for the suppliers to process.

If the calls do not have a return value this description is enough,
however if a logged call has a return value,
there must be some policy on how to deal with it.
One possibility is to use futures and promises to represent the value that has
not yet been computed,
another is to wait for the result of every asynchronous call.
The first option offers more asynchronous behaviour,
but introduces more design questions such as:

\begin{itemize}
\item when should the future be evaluated?
\item how should futures be combined (i.e., adding two result promises)?
\end{itemize}
We do not contribute anything to deal with these questions;
the E programming language~\cite{miller:2006:robust_composition}
uses promises extensively in this way.

This work uses the second option:
waiting for the result before proceeding.
It is not as asynchronous, but allows for a simpler execution model,
and more optimization opportunities later.

%}}}

%{{{ Implementation
\section{Implementation}
\subsection{High level design}
The high level design that we use is that each processor is
represented by a queue.
This queue corresponds directly to the ``stream'' of computations
that was mentioned previously.
Additionally each processor also has a mechanism to
guarantee exclusive access to the queue.
This way,
clients can ensure that when they have access,
they will be able to maintain pre/postcondition reasoning
between the successive calls in places in the supplier's queue.

\tikzstyle{queue_block} =
  [ draw
  , fill=blue!20
  , minimum size = 1em
  ]

\def\queueAt#1#2#3{
  \foreach \x in {0, 1,...,4} {
    \node[queue_block] at (#1 em + \x em, #2 em) (queue_block_\x_#3) {};
  }
}
\begin{center}
  \begin{tikzpicture}
    \queueAt{0}{1}{0}
  \end{tikzpicture}
\end{center}
\subsection{Refining the design}
The above high level design is one which is able to provide
the guarantees that are required of the programming model.
It does, however, contain many things which make the
direct implementation of the design inefficient.
For example,
using a mutex to maintain exclusive access to the queue
has two problems:

\begin{enumerate}
\item Clients face mutex contention on every access to the queue,
  possibly requiring an expensive context switch.
\item Since there is a single queue, this increases memory contention between
  the different processors.
\end{enumerate}

To reduce these effects,
it is more efficient to introduce local queues which each client
places their closures into for a particular supplier.
That local queue is then given to the supplier,
which maintains a queue of queues.
\begin{center}
  \begin{tikzpicture}

    \foreach \y in {0, 1,..., 4} { \queueAt{2}{2*\y}{\y} }

    \foreach \y in {0, 1,..., 4} { \node[queue_block, fill=red!20] at
      (0, \real{2}*\y em) (main_block_\y) {}; }

    \foreach \from/\to in {0/1, 1/2, 2/3, 3/4} { \draw
      (main_block_\from) -- (main_block_\to); }

    \foreach \y in {0, 1,..., 4} { \path[draw, thick] (main_block_\y)
      -- (queue_block_0_\y); }
  \end{tikzpicture}
\end{center}
To prevent the queues from growing without limit,
a bound is placed on the number of elements they can handle.
This means that some synchronization may still be necessary,
but it only occurs in the slow path (only when the queue is full).
The fast path, though, is quite efficient,
only requiring a single atomic exchange and increment operation
to implement.
The mutex alternative requires at least one atomic operation in the fast path,
and a context switch in the slow path.
However, the slow path is encountered by $n - 1$ participants if
$n$ clients are all vying for the same supplier.
%}}}

%{{{ Evaluation, performance
\section{Evaluation, performance}

\subsection{Design evaluation}

\subsection{Computation}

\subsection{Coordination}
%}}}

%{{{ Related work
\section{Related work}
\subsection{Work stealing}

Work stealing~\cite{blumofe:1994:scheduling} assumes the scheduling
forms a DAG. 
We tolerate some cyclic schedules through the use of queues.
Since we use queues, processor A can log work on processor B at the 
same time processor B logs work on A, as long as there is room in theirr
espective queues, and they do not issue queries on one another
(forcing a join edge).
We are not strict: edges go into processors from outside other than at spawn;
this is actually the normal case (loggin commands/queries).

\textbf{Investigate for implementation:}
Each processor maintains their own queue, and work can be stolen from
other processor's queues when a processor finds their own queue empty.

\subsection{X10 Help-first}
The help-first stealing discipline~\cite{guo:2009:work} in X10
offers that pushing the async/spawn work onto the stack has benefits
as it avoids the necessity of the thiefs synchronizing.
This only applies because the work-first steals 
in a finish block in X10 are serialized, 
whereas they are not for help-first.
This seems to be a finish-specific problem/solution
that is not directly applicable to SCOOP because
SCOOP only has 'singleton' finish blocks where we have wait-by-necessity
on results of a single  processor.

Reading stack:

\begin{itemize}
\item arbitrary dependencies are impossible to schedule efficiciently:
  Space efficient scheduling of multithreaded computations [Blumofe]
\end{itemize}
%}}}

%{{{ Conclusion
\section{Conclusion}
%}}}

\bibliography{bibfile}{}
\bibliographystyle{plain}

\end{document}

;; Local variables:
;; folded-file: t
;; end: